# Glue Job - Feature Store para InferÃªncia de CrÃ©dito

Este repositÃ³rio contÃ©m a infraestrutura e o script PySpark para geraÃ§Ã£o de uma Feature Store no S3 utilizando AWS Glue com Spark puro (sem GlueContext). Todo o processo estÃ¡ modularizado e com deploy automatizado via GitHub Actions.

## ğŸ“ Estrutura do Projeto 

```
â”œâ”€â”€ infra/                          # Infraestrutura como cÃ³digo (Terraform)
â”‚   â”œâ”€â”€ main.tf                     # Recursos AWS (Glue Job, IAM, etc.)
â”‚   â”œâ”€â”€ policy/
â”‚   â”‚   â”œâ”€â”€ assume_role.json        # PolÃ­tica de trust da role do Glue
â”‚   â”‚   â””â”€â”€ glue_access_policy.json# PolÃ­tica de acesso ao S3
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ featurestore/
â”‚       â”œâ”€â”€ main.py                # Ponto de entrada do Glue Job
â”‚       â”œâ”€â”€ feature_store.py       # Classe de orquestraÃ§Ã£o do fluxo ETL
â”‚       â””â”€â”€ utils.py               # SparkSession + FeatureEngineering modularizado
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ deploy.yml                 # Pipeline de CI/CD com GitHub Actions
```

## ğŸš€ Deploy

O deploy Ã© realizado automaticamente via **GitHub Actions** ao fazer `push` na branch `main`. Ele realiza:

1. CompressÃ£o dos scripts (`main.py`, `utils.py`, `feature_store.py`)
2. Upload do `.zip` no S3 (em um bucket versionado/artifactory)
3. ExecuÃ§Ã£o do `terraform apply` na pasta `infra/` para criar/atualizar o Glue Job

---

## ğŸ“¦ ExecuÃ§Ã£o do Glue Job

O job pode ser executado de duas formas:

### ğŸ” Via Step Functions ou outro orquestrador

VocÃª pode passar os parÃ¢metros `--input_path` e `--output_path` diretamente para o Glue Job via trigger ou orquestrador:

```json
--input_path=s3://meu-bucket-transient/meus-dados/
--output_path=s3://meu-bucket-sot/feature-store/
```

### ğŸ–¥ï¸ Manualmente pelo Console AWS Glue

1. VÃ¡ atÃ© o [AWS Glue Console](https://console.aws.amazon.com/glue/).
2. Clique no Job `glue-featurestore-job`.
3. Clique em **"Run Job"**.
4. Em *Script parameters (optional)*, adicione:

```
--input_path=s3://meu-bucket-transient/meus-dados/
--output_path=s3://meu-bucket-sot/feature-store/
```

---

## âœ… PrÃ©-requisitos Locais

Antes de rodar o pipeline:

- Ter [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html) configurado (`aws configure`)
- Ter [Terraform](https://developer.hashicorp.com/terraform/downloads) instalado
- Ter as variÃ¡veis de ambiente do GitHub configuradas:
  - `AWS_ACCESS_KEY_ID`
  - `AWS_SECRET_ACCESS_KEY`

---

## ğŸ§  O que este job faz?

- LÃª arquivos `.csv` da camada **transient**
- Realiza diversas transformaÃ§Ãµes e categorizaÃ§Ã£o de dados
- Aplica *one-hot encoding* customizado
- Salva os dados tratados em `.parquet` na camada **SOT** (feature-store), particionando por data (`anomesdia`)

---

## ğŸ“Œ ObservaÃ§Ãµes

- O script Ã© compatÃ­vel com Spark puro, sem uso de `GlueContext`
- As transformaÃ§Ãµes estÃ£o modularizadas para facilitar manutenÃ§Ã£o e reuso
- Suporte completo para execuÃ§Ã£o via esteira ou console manual
